# -*- coding: utf-8 -*-
"""NLLB-200_xh.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g-1ZowwpzpFCa99biSP1QIEXHgauUQi_
"""

!pip install transformers datasets evaluate --quiet

import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# ----------------------------
# 1️⃣ Load Parallel Corpus
# ----------------------------
SRC_LANG = "eng_Latn"
TGT_LANG = "xho_Latn"

with open("/content/drive/MyDrive/English-isiXhosa/English1_cleaned_new.txt", encoding="utf-8") as f:
    en = f.read().strip().split("\n")
with open("/content/drive/MyDrive/English-isiXhosa/isiXhosa1_cleaned_new.txt", encoding="utf-8") as f:
    xh = f.read().strip().split("\n")

assert len(en) == len(xh), "❌ Mismatch in line counts!"

df = pd.DataFrame({"en": en, "xh": xh})
raw = Dataset.from_pandas(df)
dataset = raw.train_test_split(test_size=0.1)

print(f"✅ Dataset loaded: {len(dataset['train'])} train / {len(dataset['test'])} test samples")

model_name = "facebook/nllb-200-distilled-600M"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Check if target language is in tokenizer's vocabulary
if tokenizer.convert_tokens_to_ids(TGT_LANG) is None:
    raise ValueError(f"Target language '{TGT_LANG}' not found in tokenizer vocabulary.")

tokenizer.src_lang = SRC_LANG
print(f"Using source language: {SRC_LANG}")
print(f"Using target language: {TGT_LANG}")


# Get forced BOS token id for isiZulu
forced_bos_token_id = tokenizer.convert_tokens_to_ids(TGT_LANG)
print("✅ Forced BOS Token ID:", forced_bos_token_id)

def preprocess(batch):
    src_texts = batch["en"]
    tgt_texts = batch["xh"]

    model_inputs = tokenizer(
        src_texts,
        max_length=128,
        truncation=True,
        padding="max_length"
    )

    # Explicitly set target language for tokenizer
    tokenizer.tgt_lang = TGT_LANG
    labels = tokenizer(
        text_target=tgt_texts,
        max_length=128,
        truncation=True,
        padding="max_length"
    )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized = dataset.map(preprocess, batched=True)
print("✅ Tokenization complete.")

import evaluate
import numpy as np

bleu = evaluate.load("bleu")

def compute_bleu(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]

    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    result = bleu.compute(predictions=decoded_preds, references=decoded_labels)
    return {"bleu": result["bleu"]}

import torch
torch.cuda.empty_cache()

from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq

training_args = Seq2SeqTrainingArguments(
    output_dir="./nllb_xhosa_model",
    eval_strategy="epoch",
    learning_rate=3e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    weight_decay=0.01,
    num_train_epochs=5,
    save_total_limit=2,
    predict_with_generate=True,
    logging_strategy="steps",
    logging_steps=10,
    generation_max_length=128,
    fp16=True
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_bleu
)

trainer.train()

results = trainer.evaluate()
print("✅ BLEU Score:", results["eval_bleu"])

model.save_pretrained("nllb-xhosa-finetuned")
tokenizer.save_pretrained("nllb-xhosa-finetuned")
print("✅ Model saved successfully.")

text = "My family lives in the Eastern Cape province."

inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True).to(model.device)
translated_tokens = model.generate(
    **inputs,
    forced_bos_token_id=forced_bos_token_id,
    max_length=128,
    num_beams=5
)

translated = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)
print("✅ Translated isiXhosa:", translated)